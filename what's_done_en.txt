1) Using groupby and decode_rle, I created a folder named masks where all masks are stored based on the CSV file. The code that converted RLE strings into images was saved in RLE_decoding.py.

2) I created a model to test whether it would perform better with fewer filters and a deeper architecture than the classical U-Net, using Google Cloud to train it. The code for data loading, model construction, and training was saved in model.py.

3) The model was trained on Google Cloud using simple python model.py and screen to run everything in the background without my intervention.

4) However, I encountered several issues (accuracy was xxx to the power of -7). I debugged the model and fixed the errors. I found that input image normalization wasn't working properly, and the dataset was incomplete. I added BatchNormalization at each stage and replaced layers with SeparableConv2D. As a result, the model had 1 million parameters instead of two million.

5) I started a new training session, but there was an issue with the masks. On the second epoch, I achieved a loss of 0.033. It seems like I need to use a different loss function and train the model only on data with ships. I also plan to train another model to detect whether ships are present in the image.

6) I modified the RLE-decoding function to create a file only if there is a ship. I ended up with 46,000 images/masks. I adjusted the metrics for accuracy and loss and resolved issues with data type conversion for masks. I also resolved issues with tf_image.convert_image_dtype(). Now I need to create a model to determine whether there is a ship in the image.

7) I trained the segmentation model and achieved a val_Dice_coef of 0.75 after 40 epochs. I'll need to come up with something and figure out how to train it better because I'd like to improve further.

8) I created a model for ship detection, which required creating a new dataset. The code for dataset creation is in Dataset_for_classification.py, while data loading, model creation, and training for ship detection are in Classification_model.py.

9) I made some mistakes with the dimensions in the detection model, resulting in a model with 112 million parameters that didn't want to train. I reduced the number of filters and Dense layers, and everything seemed to work fine.

10) I found another way to calculate the combined dice/cross-entropy loss. I need to try retraining the segmentation model using all 62,000 images for training/validation and modify dice_p_bce according to the code here: [link to loss_functions.py](https://github.com/shruti-jadon/Semantic-Segmentation-Loss-Functions/tree/master/loss_functions.py).

11) In the main.py file, I wrote code to implement the entire process (loading image -> ship detection -> segmentation if a ship is found). The results are more or less okay, but they could be better.

12) I decided to retrain the model using augmented data, a new loss function, and training on the entire dataset. The code for implementing augmentation is saved in data_augmentation.py.

13) Something went wrong with the training, probably overfitting.

14) It turns out it wasn't overfitting, but rather a mistake in the data augmentation process where I accidentally loaded input images into both images and masks. I fixed the code, recreated the dataset, trained the model with the new loss function, and it performed better, even without ship detection.